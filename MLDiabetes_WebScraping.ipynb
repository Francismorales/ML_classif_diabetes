{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandasql as ps\n",
    "\n",
    "#libraries for web scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import time\n",
    "import random\n",
    "\n",
    "#libraries to connect to API\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load API Key locally stored\n",
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import reference data to the notebook\n",
    "\n",
    "#This is the list of counties for the study\n",
    "county_selection = pd.read_csv(\"MENUISM3/county_selection.csv\")\n",
    "\n",
    "#This document will be used in several modules through the notebook\n",
    "allzipUSA = pd.read_csv(\"MENUISM3/zip_state_county.csv\") #import document with state/county/zipcode info\n",
    "allzipUSA['zipcode'] = allzipUSA['zipcode'].astype(str) #zipcode as string type\n",
    "allzipUSA['zipcode'] = allzipUSA['zipcode'].apply(lambda x: '{0:0>5}'.format(x)) #add leading zeros to len 4 zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McDonald's Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has the data gathering process of the number of McDonalds in a sample of counties. Below the general steps followed during this notebook:\n",
    "\n",
    "1. Select a 827 counties for the study (based on ASC county list)\n",
    "2. Find the zipcodes corresponding to those 827 counties\n",
    "3. Use Web Scrapping to find the addresses of the near by McDonald's for each zipcode \n",
    "4. Use Google Places API to get the correct zipcode and to confirm if the restaurant is still operating of each restaurant retrieved by the web scrapping process \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get zipcodes of selected counties for the study\n",
    "McDonaldsZipCodes = county_selection.merge(allzipUSA, how='left', on=['state_abbrev', 'state_name','county'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scrapping will be used to get the \"near by\" McDonald's restaurants by zipcode. The zipcodes to search correspond to the ones from the selected counties for this study.\n",
    "\n",
    "This web scrapping process does not return the zipcode or county corresponding to the McDonald's address. A second step using Google Maps API will help me confirm zipcode and county. \n",
    "\n",
    "**Menuism Website:** https://www.menuism.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24107"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a list of zipcodes to the web scrapping routine\n",
    "web_scrapping_list = list(McDonaldsZipCodes['zipcode'])\n",
    "len(web_scrapping_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scrapping tips:\n",
    "- Use different user-agents for each web scrapping round\n",
    "- Delete cookies after every web scrapping round\n",
    "- Re-start the kernel/open a new jupyter notebook session after every web scrapping round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/534.55.3 (KHTML, like Gecko) Version/5.1.3 Safari/534.53.10\"\n",
    "#     user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.79 Safari/537.36'\n",
    "#     user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   36507\n",
      "1   36511\n",
      "2   36526\n",
      "3   36527\n",
      "4   36530\n"
     ]
    }
   ],
   "source": [
    "#This is a routine for web scraping McDonald's restarurants by zip code \n",
    "#Web page: Menuism.com\n",
    "#Python libraries: urllib.request, beautiful soup and pandas\n",
    "\n",
    "scrapping_list = web_scrapping_list[0:5] #divide full list into smaller ranges\n",
    "alldata = pd.DataFrame(columns=['name', 'address', 'city', 'zipcode']) #initialize dataframe\n",
    "for j, zipi in enumerate(scrapping_list):\n",
    "    url = \"https://www.menuism.com/search?q=McDonald%27s&l=\" + zipi + \"&p=0&s=match&pp=25&f=0\"\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    page = urllib.request.urlopen(request) #load the web page\n",
    "    soup = BeautifulSoup(page, 'html.parser') #parse HTML document\n",
    "    addresses = soup.find_all(\"em\") #HTML elements & attributes\n",
    "    cities = soup.find_all(\"a\", {\"onclick\":\"_gaq.push(['_trackEvent', 'RlRow', 'Click', 'City', null, false]);\"})\n",
    "    names = soup.find_all(\"a\", {\"onclick\":\"_gaq.push(['_trackEvent', 'RlRow', 'Click', 'RL Name', null, false]);\"})\n",
    "    if len(cities) > 0:  #if no results retrieved from web page, skip to else\n",
    "        lsaddress, lscities, lszipcode, lsnames = [], [], [], []\n",
    "        for i in range(len(addresses)):\n",
    "            lsaddress.append(addresses[i].text) \n",
    "        lsaddress.remove(lsaddress[0]) #this is specifically for MENUISM web scrapping. Found after inspection 1st item is garbage\n",
    "        for i in range(len(cities)): #cities and names always have the same length\n",
    "            lscities.append(cities[i].text)\n",
    "            lsnames.append(names[i].text)\n",
    "            lszipcode.append(zipi)\n",
    "        dctionary = {'name':lsnames,'address':lsaddress, 'city':lscities, 'zipcode': lszipcode} #store data to dictionary\n",
    "        alldata = pd.concat([alldata,pd.DataFrame(dctionary)], ignore_index=True, sort=False) #transfer data to pandas dataframe\n",
    "    else: \n",
    "        dctionary = {'name':['No_result'],'address':['No_result'], 'city':['No_result'], 'zipcode':[zipi]}  #store collected data to dictionary\n",
    "        alldata = pd.concat([alldata,pd.DataFrame(dctionary)], ignore_index=True, sort=False)\n",
    "    print(j, \" \" , zipi) #for visual control\n",
    "    time.sleep(random.randint(1, 6))  #sleep to ensure IP address is not blocked\n",
    "alldata.to_csv('Web_scraping_McDonalds.csv') #save web scrapping data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save web scrapping data to csv file\n",
    "alldata.to_csv('MENUISM3/3500_6808MENUISM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Places API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Places API to confirm address from web scrapping and check if the restaurant is still operating. \n",
    "\n",
    "Google Places API: https://console.cloud.google.com/google/maps-apis/overview?project=peak-apparatus-272919\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleanning prior to API requests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\franc\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\franc\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "#import web scrapping data\n",
    "MENUISM1 = pd.read_csv(\"MENUISM3/0_3500MENUISM.csv\")\n",
    "MENUISM2 = pd.read_csv(\"MENUISM3/3500_6808MENUISM.csv\")\n",
    "\n",
    "MENUISM = pd.concat([MENUISM1, MENUISM2])\n",
    "\n",
    "MENUISM = MENUISM.loc[MENUISM['name'] != 'No_result']\n",
    "\n",
    "#convert zipcode column to str type\n",
    "MENUISM['zipcode'] = MENUISM['zipcode'].astype(str)\n",
    "#add leading zeros to zipcode\n",
    "MENUISM['zipcode'] = MENUISM['zipcode'].apply(lambda x: '{0:0>5}'.format(x))\n",
    "\n",
    "# Find a duplicate rows from web scrapped data\n",
    "# Find duplicates on address & city (remember that I put the web scrapping zipcode, might not be the actual zipcode)\n",
    "# so I could have multiple duplicate addresses returned by different zipcodes due to the nearby reults\n",
    "duplicateDFRow = MENUISM[MENUISM.duplicated(['address', 'city'], keep='first')]\n",
    "#create a dataframe with unique McDonalds addresses to later confirm with Google places API\n",
    "keys = ['address','city', 'zipcode']\n",
    "i1 = MENUISM.set_index(keys).index\n",
    "i2 = duplicateDFRow.set_index(keys).index\n",
    "MENUISM_unique = MENUISM[~i1.isin(i2)] #this is the dataframe to use for Google API\n",
    "\n",
    "#Split city column by comma\n",
    "city = MENUISM_unique['city'].str.split(', ', expand=True)\n",
    "MENUISM_unique['city'] = city[0] \n",
    "MENUISM_unique['state'] = city[1] \n",
    "\n",
    "MENUISM_unique.drop(labels=['Unnamed: 0','Unnamed: 0.1'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>1901 Quintard Ave</td>\n",
       "      <td>Anniston</td>\n",
       "      <td>36201</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>3424 Greenbrier Dear Rd</td>\n",
       "      <td>Anniston</td>\n",
       "      <td>36201</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>5550 Mcclellan Blvd</td>\n",
       "      <td>Anniston</td>\n",
       "      <td>36201</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>1015 Quintard Dr</td>\n",
       "      <td>Oxford</td>\n",
       "      <td>36203</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>92 Plaza Ln</td>\n",
       "      <td>Oxford</td>\n",
       "      <td>36203</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17313</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>11961 Asheville Hwy</td>\n",
       "      <td>Inman</td>\n",
       "      <td>29348</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17344</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>308 W Wade Hampton Blvd</td>\n",
       "      <td>Greer</td>\n",
       "      <td>29652</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17345</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>103 Hammett Bridge Rd</td>\n",
       "      <td>Greer</td>\n",
       "      <td>29652</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17346</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>2137 Old Spartanburg Rd</td>\n",
       "      <td>Greer</td>\n",
       "      <td>29652</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17347</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Old Spartanburg Rd</td>\n",
       "      <td>Greer</td>\n",
       "      <td>29652</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2457 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                  address      city zipcode state\n",
       "0      McDonald's        1901 Quintard Ave  Anniston   36201    AL\n",
       "1      McDonald's  3424 Greenbrier Dear Rd  Anniston   36201    AL\n",
       "2      McDonald's      5550 Mcclellan Blvd  Anniston   36201    AL\n",
       "6      McDonald's         1015 Quintard Dr    Oxford   36203    AL\n",
       "7      McDonald's              92 Plaza Ln    Oxford   36203    AL\n",
       "...           ...                      ...       ...     ...   ...\n",
       "17313  McDonald's      11961 Asheville Hwy     Inman   29348    SC\n",
       "17344  McDonald's  308 W Wade Hampton Blvd     Greer   29652    SC\n",
       "17345  McDonald's    103 Hammett Bridge Rd     Greer   29652    SC\n",
       "17346  McDonald's  2137 Old Spartanburg Rd     Greer   29652    SC\n",
       "17347  McDonald's       Old Spartanburg Rd     Greer   29652    SC\n",
       "\n",
       "[2457 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the dataframe to use for the Google API requests\n",
    "MENUISM_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google API Requests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import API Key from local directory\n",
    "#API keys are sensitive and should not be hard coded to your notebook\n",
    "keys = get_keys(\"/Users/franc/.secret/GoogleAPI2.json\")\n",
    "api_key = keys['api_key'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists for loop\n",
    "addres_list = list(MENUISM_unique['address'])\n",
    "state_list = list(MENUISM_unique['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Google API: Places API: findplacefromtext\n",
    "placesAPI_data = pd.DataFrame(columns=['formatted_address', 'name', 'permanently_closed'])\n",
    "for i in range(len(addres_list)):\n",
    "    address = addres_list[i].replace(' ', '%20') #make sure there are no blank spaces for the URL\n",
    "    state = state_list[i]\n",
    "    address_search = \"McDonald's%20\"+ address + \",%20\" + state + \",%20USA\"\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json?input=\"+ address_search + \\\n",
    "        \"&inputtype=textquery&fields=name,formatted_address,permanently_closed&key=\"+ api_key \n",
    "    response = requests.get(url).json()\n",
    "    placesAPI_data = pd.concat([placesAPI_data, pd.DataFrame(response['candidates'])], ignore_index=True, sort=False) #append retrieved information to a dataframe\n",
    "    time.sleep(1.25)\n",
    "    print(i, \" \" , address_search) #print for visual control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placesAPI_data.to_csv('MENUISM3/GooglePlacesAPIOutput.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placesAPI_data = pd.read_csv(\"MENUISM3/GooglePlacesAPIOutput.csv\")\n",
    "\n",
    "# #Split address column by comma, add name and perm_closed colums and rename split columns\n",
    "McDonaldsdata = placesAPI_data['formatted_address'].str.split(', ', expand=True)\n",
    "McDonaldsdata['name'] = placesAPI_data['name'] \n",
    "McDonaldsdata['permanently_closed'] = placesAPI_data['permanently_closed'] \n",
    "McDonaldsdata.rename(columns={0: \"address\", 1: \"city\", 2: \"zipcode\", 3: \"country\"}, inplace=True)\n",
    "\n",
    "#drop duplicate rows\n",
    "McDonaldsdata.drop_duplicates(['address', 'zipcode'],keep='first',inplace=True) \n",
    "\n",
    "#delete rows with permanently closed = True\n",
    "rowsdrop = McDonaldsdata.loc[McDonaldsdata['permanently_closed'] == True]\n",
    "McDonaldsdata = McDonaldsdata.drop(rowsdrop.index)\n",
    "\n",
    "#Drop rows with names different than the ones in the list\n",
    "rowsdropname = McDonaldsdata.loc[~McDonaldsdata['name'].isin([\"McDonald's\", \"McDonalds\", \"Walmart Supercenter\", \"Walmart\", \"McDonald’s\"])]\n",
    "McDonaldsdata = McDonaldsdata.drop(rowsdropname.index)\n",
    "\n",
    "#Arrange the info in the correct columns for rows with country == Estados Unidos in column 4\n",
    "McDonaldsdata.loc[McDonaldsdata[4] == 'Estados Unidos','address'] = McDonaldsdata['address'] + ', ' + McDonaldsdata['city']\n",
    "McDonaldsdata.loc[McDonaldsdata[4] == 'Estados Unidos','city'] = McDonaldsdata['zipcode']\n",
    "McDonaldsdata.loc[McDonaldsdata[4] == 'Estados Unidos','zipcode'] = McDonaldsdata['country']\n",
    "McDonaldsdata.loc[McDonaldsdata[4] == 'Estados Unidos','country'] = McDonaldsdata[4]\n",
    "\n",
    "#Split zipcode into zipcode and state code\n",
    "McDonaldsdata[['state_abbrev', 'zipcode', 'extra']] = McDonaldsdata['zipcode'].str.split(\" \",expand=True)\n",
    "\n",
    "#Drop rows with names different than the ones in the list\n",
    "rowsdropcountry = McDonaldsdata.loc[~McDonaldsdata['country'].isin([\"Estados Unidos\"])]\n",
    "McDonaldsdata = McDonaldsdata.drop(rowsdropcountry.index)\n",
    "\n",
    "McDonaldsdata.drop(labels=[4,'permanently_closed', 'extra'], inplace=True, axis=1)\n",
    "\n",
    "#count number of McDonalds by zipcode\n",
    "McDonaldsdataFinal = McDonaldsdata.groupby(['state_abbrev', 'zipcode'], as_index=False)['address'].count()\n",
    "McDonaldsdataFinal.rename(columns={'address': \"McDonaldsCount\"}, inplace=True)\n",
    "\n",
    "#add county to each row based on zipcode. Get zipcode from external file\n",
    "McDonaldsdataFinal = McDonaldsdataFinal.merge(allzipUSA, how='left', on=['state_abbrev','zipcode'])\n",
    "\n",
    "McDonaldsdataFinal = McDonaldsdataFinal.groupby(['state_abbrev', 'county'], as_index=False)['McDonaldsCount'].sum()\n",
    "\n",
    "#exclude the counties that were not in the search list. This because it is likely that the data from\n",
    "#counties that were not in the original list is a result of the \"nearby\" restaurants and results are incomplete \n",
    "#get the working dataset with zipcodes for the web scrapping process\n",
    "keys = ['state_abbrev','county']\n",
    "i1 = McDonaldsZipCodes.set_index(keys).index\n",
    "i2 = McDonaldsdataFinal.set_index(keys).index\n",
    "McDonaldsdataFinal = McDonaldsdataFinal[i2.isin(i1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the dataset to use in the study\n",
    "McDonaldsdataFinal.to_csv('MENUISM3/COMPLETE238McDonaldsCounties.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import reference data to the notebook\n",
    "#This is the list of counties for the study\n",
    "county_selection = pd.read_csv(\"MENUISM3/county_selection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create lists for loop\n",
    "county_name = list(county_selection['county'])\n",
    "state_name = list(county_selection['state_name'])\n",
    "len(state_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscr_county = county_name[787:823]\n",
    "webscr_state = state_name[787:823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/534.55.3 (KHTML, like Gecko) Version/5.1.3 Safari/534.53.10\"\n",
    "#     user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.79 Safari/537.36'\n",
    "#     user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0'\n",
    "user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "randomtimes = [0.5, 0.75] #these are seconds for the sleep method\n",
    "df = pd.DataFrame(columns=['county', 'state_name', 'month', 'high/low', 'rain']) #initialize dataframe\n",
    "for j,zipi in enumerate(webscr_county):\n",
    "    county = webscr_county[j].replace(' ', '%20')\n",
    "    state = webscr_state[j] .replace(' ', '%20')\n",
    "    url = \"https://www.google.com/search?q=Average+monthly+temperature+\" + county + \"+county+\" + state\n",
    "    user_agent =  'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    page = urllib.request.urlopen(request)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    month = soup.find_all(\"span\", {\"role\":\"gridcell\"})\n",
    "    print(j)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(month):\n",
    "        txtmonth = month[i].text\n",
    "        txttemp = month[i+1].text\n",
    "        txtrain = month[i+2].text\n",
    "        dctionary = {'county': [webscr_county[j]],'state_name': [webscr_state[j]],'month':[txtmonth],'high/low':[txttemp], 'rain':[txtrain]}\n",
    "        df = pd.concat([df,pd.DataFrame(dctionary)], ignore_index=True, sort=False)\n",
    "        i = i + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('MENUISM3/temperaturedata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import web scrapped data\n",
    "temperature_data = pd.read_csv(\"MENUISM3/Temperature_rain_data.csv\")\n",
    "temperature_data.drop(labels=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True, axis=1) #drop unnecessary columns\n",
    "temperature_data.drop_duplicates(keep='first', inplace=True) #drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>state_name</th>\n",
       "      <th>month</th>\n",
       "      <th>high/low</th>\n",
       "      <th>rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>mayo</td>\n",
       "      <td>29° / 17°</td>\n",
       "      <td>6 días</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>junio</td>\n",
       "      <td>32° / 21°</td>\n",
       "      <td>9 días</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>julio</td>\n",
       "      <td>33° / 22°</td>\n",
       "      <td>11 días</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>agosto</td>\n",
       "      <td>32° / 22°</td>\n",
       "      <td>11 días</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>enero</td>\n",
       "      <td>16° / 4°</td>\n",
       "      <td>8 días</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13095</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>agosto</td>\n",
       "      <td>34° / 20°</td>\n",
       "      <td>1 día</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13096</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>septiembre</td>\n",
       "      <td>28° / 14°</td>\n",
       "      <td>2 días</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13097</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>octubre</td>\n",
       "      <td>19° / 7°</td>\n",
       "      <td>2 días</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13098</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>noviembre</td>\n",
       "      <td>11° / 1°</td>\n",
       "      <td>2 días</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13099</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>diciembre</td>\n",
       "      <td>5° / -4°</td>\n",
       "      <td>2 días</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9884 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        county state_name       month   high/low     rain\n",
       "0      Baldwin    Alabama        mayo  29° / 17°   6 días\n",
       "1      Baldwin    Alabama       junio  32° / 21°   9 días\n",
       "2      Baldwin    Alabama       julio  33° / 22°  11 días\n",
       "3      Baldwin    Alabama      agosto  32° / 22°  11 días\n",
       "4      Baldwin    Alabama       enero   16° / 4°   8 días\n",
       "...        ...        ...         ...        ...      ...\n",
       "13095   Tooele       Utah      agosto  34° / 20°    1 día\n",
       "13096   Tooele       Utah  septiembre  28° / 14°   2 días\n",
       "13097   Tooele       Utah     octubre   19° / 7°   2 días\n",
       "13098   Tooele       Utah   noviembre   11° / 1°   2 días\n",
       "13099   Tooele       Utah   diciembre   5° / -4°   2 días\n",
       "\n",
       "[9884 rows x 5 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>state_name</th>\n",
       "      <th>month_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Chelan</td>\n",
       "      <td>Washington</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         county  state_name  month_count\n",
       "133      Chelan  Washington           10\n",
       "726  Terrebonne   Louisiana           10"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if any county does not have 12 months\n",
    "month_check = temperature_data.groupby(['county', 'state_name'], as_index=False)['month'].count()\n",
    "month_check.rename(columns={'month':'month_count'}, inplace=True)\n",
    "\n",
    "month_check.loc[month_check['month_count'] != 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>state_name</th>\n",
       "      <th>month</th>\n",
       "      <th>high/low</th>\n",
       "      <th>rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4464</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>mayo</td>\n",
       "      <td>29° / 20°</td>\n",
       "      <td>junio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4465</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>32° / 23°</td>\n",
       "      <td>julio</td>\n",
       "      <td>32° / 24°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4466</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>agosto</td>\n",
       "      <td>33° / 24°</td>\n",
       "      <td>enero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4467</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>17° / 8°</td>\n",
       "      <td>febrero</td>\n",
       "      <td>19° / 10°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>marzo</td>\n",
       "      <td>22° / 12°</td>\n",
       "      <td>abril</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4469</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>25° / 16°</td>\n",
       "      <td>mayo</td>\n",
       "      <td>29° / 20°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4470</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>junio</td>\n",
       "      <td>32° / 23°</td>\n",
       "      <td>julio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>32° / 24°</td>\n",
       "      <td>agosto</td>\n",
       "      <td>33° / 24°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4472</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>septiembre</td>\n",
       "      <td>31° / 22°</td>\n",
       "      <td>octubre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>Terrebonne</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>27° / 16°</td>\n",
       "      <td>noviembre</td>\n",
       "      <td>22° / 11°</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          county state_name       month   high/low       rain\n",
       "4464  Terrebonne  Louisiana        mayo  29° / 20°      junio\n",
       "4465  Terrebonne  Louisiana   32° / 23°      julio  32° / 24°\n",
       "4466  Terrebonne  Louisiana      agosto  33° / 24°      enero\n",
       "4467  Terrebonne  Louisiana    17° / 8°    febrero  19° / 10°\n",
       "4468  Terrebonne  Louisiana       marzo  22° / 12°      abril\n",
       "4469  Terrebonne  Louisiana   25° / 16°       mayo  29° / 20°\n",
       "4470  Terrebonne  Louisiana       junio  32° / 23°      julio\n",
       "4471  Terrebonne  Louisiana   32° / 24°     agosto  33° / 24°\n",
       "4472  Terrebonne  Louisiana  septiembre  31° / 22°    octubre\n",
       "4473  Terrebonne  Louisiana   27° / 16°  noviembre  22° / 11°"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect the counties with differences \n",
    "temperature_data.loc[temperature_data['county'] == 'Chelan']\n",
    "temperature_data.loc[temperature_data['county'] == 'Terrebonne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop data after inspection\n",
    "drop_rows = temperature_data.loc[temperature_data['county'].isin(['Chelan', 'Terrebonne'])]\n",
    "temperature_data = temperature_data.drop(drop_rows.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>county</th>\n",
       "      <th>state_name</th>\n",
       "      <th>state_abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, county, state_name, state_abbrev]\n",
       "Index: []"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if I have all the info for the seleted counties\n",
    "keys = ['county', 'state_name']\n",
    "i1 = month_check.set_index(keys).index\n",
    "i2 = county_selection.set_index(keys).index\n",
    "missing = county_selection[~i2.isin(i1)]\n",
    "\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split temperature in high/low\n",
    "temp = temperature_data['high/low'].str.split('/', expand=True)\n",
    "temperature_data['highCelsius'] = temp[0] \n",
    "temperature_data['lowCelsius'] = temp[1]\n",
    "\n",
    "#remove the degree sign from temperature\n",
    "temperature_data['highCelsius'] = temperature_data.apply(lambda x: x['highCelsius'].strip(), axis=1)\n",
    "temperature_data['lowCelsius'] = temperature_data.apply(lambda x: x['lowCelsius'].strip(), axis=1)\n",
    "\n",
    "temperature_data['highCelsius'] = temperature_data.apply(lambda x: x['highCelsius'][:-1], axis=1)\n",
    "temperature_data['lowCelsius'] = temperature_data.apply(lambda x: x['lowCelsius'][:-1], axis=1)\n",
    "\n",
    "#remove 'dias' word from rain days\n",
    "temperature_data['rain'] = temperature_data.apply(lambda x: x['rain'].strip(), axis=1)\n",
    "temperature_data['rain'] = temperature_data.apply(lambda x: x['rain'][:-5], axis=1)\n",
    "temperature_data.rename(columns={'rain': \"days_of_rain\"}, inplace=True)\n",
    "\n",
    "#remove columns\n",
    "temperature_data.drop(labels=['high/low'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_data['month'].replace({\"enero\": \"January\", \"febrero\": \"February\", \"marzo\": \"March\", \"abril\": \"April\", \"mayo\": \"May\", \"junio\": \"June\", \"julio\": \"July\", \"agosto\": \"August\", \"setiembre\": \"September\", \"octubre\": \"October\", \"noviembre\": \"November\", \"diciembre\": \"December\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export data to folder\n",
    "temperature_data.to_csv(\"DATA\\Rain&Temperaturedata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>state_name</th>\n",
       "      <th>month</th>\n",
       "      <th>days_of_rain</th>\n",
       "      <th>highCelsius</th>\n",
       "      <th>lowCelsius</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>May</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>June</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>July</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>August</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>January</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13095</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>August</td>\n",
       "      <td></td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13096</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>septiembre</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13097</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>October</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13098</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>November</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13099</th>\n",
       "      <td>Tooele</td>\n",
       "      <td>Utah</td>\n",
       "      <td>December</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9864 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        county state_name       month days_of_rain highCelsius lowCelsius\n",
       "0      Baldwin    Alabama         May            6          29         17\n",
       "1      Baldwin    Alabama        June            9          32         21\n",
       "2      Baldwin    Alabama        July           11          33         22\n",
       "3      Baldwin    Alabama      August           11          32         22\n",
       "4      Baldwin    Alabama     January            8          16          4\n",
       "...        ...        ...         ...          ...         ...        ...\n",
       "13095   Tooele       Utah      August                       34         20\n",
       "13096   Tooele       Utah  septiembre            2          28         14\n",
       "13097   Tooele       Utah     October            2          19          7\n",
       "13098   Tooele       Utah    November            2          11          1\n",
       "13099   Tooele       Utah    December            2           5         -4\n",
       "\n",
       "[9864 rows x 6 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# County Coordinates Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code gets the latitude and longitude of the location centroids of each county of the USA. These coordinates will be used for visualizations. \n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/User:Michael_J/County_table\n",
    "\n",
    "Data was gathered by using web scraping via BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load HTML contents of webpage to variable using requests and beautifulsoup libraries\n",
    "url = \"https://en.wikipedia.org/wiki/User:Michael_J/County_table\"\n",
    "user_agent =  'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0'\n",
    "request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "page = urllib.request.urlopen(request)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "row2 = soup.find_all(\"td\") #HTML element containing table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to pandas dataframe\n",
    "county_coord = pd.DataFrame(columns=['state_abbrev', 'county', 'latitude', 'longitude'])\n",
    "for i in range(1, len(row2), 14):    \n",
    "    diction = {'state_abbrev': [row2[i].text], 'county': [row2[i+2].text], 'latitude': [row2[i+11].text], 'longitude': [row2[i+12].text]}\n",
    "    county_coord = pd.concat([county_coord, pd.DataFrame(diction)], ignore_index=True, sort=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data \n",
    "county_coord['longitude'] = county_coord['longitude'].str[:-2]\n",
    "county_coord['latitude'] = county_coord['latitude'].str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "county_coord.to_csv(\"DATA\\county_latit&longit_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221.858px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
